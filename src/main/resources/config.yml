# Chat command cooldown in milliseconds.
cooldown: 30000

# Controls the display of errors.
show-errors: false


# All the messages support formatting.
# For more info, check out https://docs.advntr.dev/minimessage/format.html.
messages:
  # These messages are shown:

  # when the gpt command is executed.
  generating: "<dark_gray>Generating, please wait..."

  # when the newchat command is executed.
  chat-cleared: "<dark_gray>This chat has been wiped."

  # when the configuration is reloaded.
  reload: "<dark_gray>Configuration successfully reloaded."

  # This message is shown before the response.
  # Formatting is preserved to the response.
  prefix: ""

  # when an error occurs.
  error: "<dark_red>Something went wrong."
  # when the cooldown is not over.
  cooldown: "<dark_red>Wait <time> seconds to use this command again."
  # when the token isn't set.
  no-token: "<dark_red>The OpenAI API token isn't set. Please contact this server's administration."


# Settings for ChatGPT.
# For more information visit https://platform.openai.com/docs/api-reference/chat/create.
chatgpt:
  # OpenAI token.
  # To get yor token, go to https://platform.openai.com/account/api-keys.
  # New accounts get 5$ for free.
  # NOTE: This is only stored locally and is not used for anything other than making requests.
  token: ""

  # ID of the model to use.
  # See https://platform.openai.com/docs/models/model-endpoint-compatibility
  # for details on which models work with the Chat API.
  model: gpt-3.5-turbo

  # What sampling temperature to use, between 0 and 2.
  # Higher values like 0.8 will make the output more random,
  # while lower values like 0.2 will make it more focused and deterministic.
  # OpenAI generally recommends altering this or top_p but not both.
  temperature: 1

  # An alternative to sampling with temperature, called nucleus sampling,
  # where the model considers the results of the tokens with top_p probability mass.
  # So 0.1 means only the tokens comprising the top 10% probability mass are considered.
  # OpenAI generally recommends altering this or temperature but not both.
  top-p: 1

  # Up to 4 sequences where the API will stop generating further tokens.
  # This only accepts a list of strings.
  stop: []

  # The maximum number of tokens to generate in the chat completion.
  # The total length of input tokens and generated
  # tokens is limited by the model's context length.
  # See https://platform.openai.com/tokenizer for more info.
  max-tokens: inf

  # Number between -2.0 and 2.0.
  # Positive values penalize new tokens based on whether
  # they appear in the text so far, increasing the
  # model's likelihood to talk about new topics.
  # See https://platform.openai.com/docs/api-reference/parameter-details for more info.
  presence-penalty: 0

  # Number between -2.0 and 2.0.
  # Positive values penalize new tokens based on their
  # existing frequency in the text so far, decreasing
  # the model's likelihood to repeat the same line verbatim.
  # See https://platform.openai.com/docs/api-reference/parameter-details for more info.
  frequency-penalty: 0
